{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcolinmorgan/human-pose-estimation-opencv/blob/master/OpenPose_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYaA0JOPSQ02",
        "outputId": "e7114a7f-d533-435d-ba87-a12b1d9e40c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'human-pose-estimation-opencv'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 31 (delta 9), reused 17 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (31/31), 16.76 MiB | 16.47 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "/content/human-pose-estimation-opencv\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dcolinmorgan/human-pose-estimation-opencv.git\n",
        "%cd /content/human-pose-estimation-opencv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CrZ9nm0USWsR",
        "outputId": "3f0e83ae-a311-4466-bdd6-1f450159433a"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m     output \u001b[38;5;241m=\u001b[39m poseDetector(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     76\u001b[0m     cv\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPose Detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     cv\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Define the body parts and their corresponding indices\n",
        "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
        "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
        "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
        "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
        "\n",
        "# Define the pairs of body parts that form a pose\n",
        "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
        "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
        "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
        "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
        "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
        "\n",
        "# Define input dimensions for the network\n",
        "width = 368\n",
        "height = 368\n",
        "inWidth = width\n",
        "inHeight = height\n",
        "\n",
        "# Load the pre-trained pose detection model\n",
        "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
        "thr = 0.2  # Confidence threshold for the detected keypoints\n",
        "\n",
        "# Function to detect poses in a frame\n",
        "def poseDetector(frame):\n",
        "    frameWidth = frame.shape[1]\n",
        "    frameHeight = frame.shape[0]\n",
        "\n",
        "    # Prepare the input blob for the network\n",
        "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
        "    out = net.forward()\n",
        "    out = out[:, :19, :, :]  # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
        "\n",
        "    assert(len(BODY_PARTS) == out.shape[1])\n",
        "\n",
        "    points = []\n",
        "    # Iterate over the body parts to extract keypoints\n",
        "    for i in range(len(BODY_PARTS)):\n",
        "        # Slice heatmap of corresponding body part\n",
        "        heatMap = out[0, i, :, :]\n",
        "\n",
        "        # Find the maximum confidence and corresponding location\n",
        "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
        "        x = (frameWidth * point[0]) / out.shape[3]\n",
        "        y = (frameHeight * point[1]) / out.shape[2]\n",
        "        points.append((int(x), int(y)) if conf > thr else None)\n",
        "\n",
        "    # Connect keypoints to form poses\n",
        "    for pair in POSE_PAIRS:\n",
        "        partFrom = pair[0]\n",
        "        partTo = pair[1]\n",
        "        assert(partFrom in BODY_PARTS)\n",
        "        assert(partTo in BODY_PARTS)\n",
        "\n",
        "        idFrom = BODY_PARTS[partFrom]\n",
        "        idTo = BODY_PARTS[partTo]\n",
        "\n",
        "        if points[idFrom] and points[idTo]:\n",
        "            # Draw line between keypoints\n",
        "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
        "            # Draw keypoints\n",
        "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "\n",
        "    t, _ = net.getPerfProfile()\n",
        "\n",
        "    return frame\n",
        "\n",
        "input = cv.imread(\"Photo5623.jpg\")\n",
        "if input is not None:\n",
        "    output = poseDetector(input)\n",
        "    cv.imshow(\"Pose Detection\", output)\n",
        "    cv.waitKey(0)\n",
        "    cv.destroyAllWindows()\n",
        "else:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "\n",
        "\n",
        "    for fn in uploaded.keys():\n",
        "        input = cv.imread(fn)\n",
        "        output = poseDetector(input)\n",
        "        cv2_imshow(output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwaz4yRrs3wz",
        "outputId": "beef6784-b27f-478b-b4fa-216e599256f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2061, 1045, 3)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ldQc8vSGEbTT"
      },
      "outputs": [],
      "source": [
        "points = []\n",
        "# Define the body parts and their corresponding indices\n",
        "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
        "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
        "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
        "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
        "\n",
        "# Define the pairs of body parts that form a pose\n",
        "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
        "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
        "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
        "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
        "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
        "\n",
        "# Define input dimensions for the network\n",
        "width = 368\n",
        "height = 368\n",
        "inWidth = width\n",
        "inHeight = height\n",
        "\n",
        "# Load the pre-trained pose detection model\n",
        "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
        "thr = 0.2  # Confidence threshold for the detected keypoints\n",
        "\n",
        "# Function to detect poses in a frame\n",
        "# def poseDetector(frame):\n",
        "frame=input\n",
        "frameWidth = frame.shape[1]\n",
        "frameHeight = frame.shape[0]\n",
        "\n",
        "net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
        "out = net.forward()\n",
        "out2 = out[:, :19, :, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su52gn57E5jF",
        "outputId": "982a1672-18e1-49b7-dfa2-b7c46e8f8786"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 57, 46, 46), (1, 19, 46, 46)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[out.shape,out2.shape]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EousXrhyE_2z"
      },
      "outputs": [],
      "source": [
        "for i in range(len(BODY_PARTS)):\n",
        "        # Slice heatmap of corresponding body part\n",
        "        heatMap = out[0, i, :, :]\n",
        "\n",
        "        # Find the maximum confidence and corresponding location\n",
        "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
        "        x = (frameWidth * point[0]) / out.shape[3]\n",
        "        y = (frameHeight * point[1]) / out.shape[2]\n",
        "        points.append((int(x), int(y)) if conf > thr else None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBz9l6FBFe-p",
        "outputId": "e9266340-3ccd-4341-ea9b-72255512bf6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(522, 358),\n",
              " (545, 537),\n",
              " (431, 582),\n",
              " (318, 761),\n",
              " None,\n",
              " (636, 537),\n",
              " None,\n",
              " None,\n",
              " (408, 985),\n",
              " (363, 1344),\n",
              " (363, 1702),\n",
              " (567, 985),\n",
              " (408, 1344),\n",
              " (567, 1568),\n",
              " (499, 358),\n",
              " (545, 358),\n",
              " (454, 403),\n",
              " (590, 403),\n",
              " (22, 1971)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YJGGKz-7FZ6v"
      },
      "outputs": [],
      "source": [
        "# Connect keypoints to form poses\n",
        "for pair in POSE_PAIRS:\n",
        "    partFrom = pair[0]\n",
        "    partTo = pair[1]\n",
        "    assert(partFrom in BODY_PARTS)\n",
        "    assert(partTo in BODY_PARTS)\n",
        "\n",
        "    idFrom = BODY_PARTS[partFrom]\n",
        "    idTo = BODY_PARTS[partTo]\n",
        "\n",
        "    if points[idFrom] and points[idTo]:\n",
        "        # Draw line between keypoints\n",
        "        cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
        "        # Draw keypoints\n",
        "        cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "        cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "\n",
        "t, _ = net.getPerfProfile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdmWtc9LFjFK",
        "outputId": "dfa14314-8b2e-4443-ccbd-e7b7f7ac338f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "63398999"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eyl7_pC6FlWa"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "import replicate\n",
        "REPLICATE_API_TOKEN = 'r8_S1mwSLhiUMaFMqEuhRzGABoQAlHn8TE2yRScT'\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = 'r8_S1mwSLhiUMaFMqEuhRzGABoQAlHn8TE2yRScT'\n",
        "\n",
        "\n",
        "input = \"https://github.com/dcolinmorgan/modegen-ai/blob/master/Photo5623.jpg?raw=true\"\n",
        "fit = \"https://github.com/dcolinmorgan/modegen-ai/blob/master/fit_pose.jpg?raw=true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "FF = replicate.models.get(\"lucataco/modelscope-facefusion\").versions.get(\"52edbb2b42beb4e19242f0c9ad5717211a96c63ff1f0b0320caa518b2745f4f7\")\n",
        "output = replicate.predictions.create(version=FF,\n",
        "        input={\n",
        "        \"user_image\": input,\n",
        "        \"template_image\": fit\n",
        "    }\n",
        ")\n",
        "output.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://replicate.delivery/pbxt/gJdekhy5L2TvBS2Aanabm7QuOmeCurZ7SzlKWoBj3QMQPUQTA/output.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url=output.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not list",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 2\u001b[0m \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/modegen/lib/python3.12/site-packages/IPython/core/display.py:923\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    921\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_ext(filename)\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 923\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_ext\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo image data found. Expecting filename, url, or data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/modegen/lib/python3.12/site-packages/IPython/core/display.py:1074\u001b[0m, in \u001b[0;36mImage._find_ext\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_find_ext\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[0;32m-> 1074\u001b[0m     base, ext \u001b[38;5;241m=\u001b[39m \u001b[43msplitext\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ext:\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m base\n",
            "File \u001b[0;32m<frozen posixpath>:118\u001b[0m, in \u001b[0;36msplitext\u001b[0;34m(p)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url=output.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "FF = replicate.models.get(\"lucataco/ip_adapter-face-inpaint\").versions.get(\"b199f118e2133894551cc59ff0777276e275cf64e9e8e0369ca6c4c599097890\")\n",
        "output = replicate.predictions.create(version=FF,\n",
        "        input={\n",
        "        \"face_image\": \"https://swedchamhk.glueup.com/resources/public/images/square/300/fa86f671-6eff-43d5-80dc-481ccfcf15e5.png\",\n",
        "        \"source_image\": \"https://fabfashionfix.com/wp-content/uploads/2013/09/Normal.jpg\",\n",
        "        \"prompt\": \"\",\n",
        "        \"strength\": 0.7,\n",
        "        \"blur_amount\": 0,\n",
        "        \"num_outputs\": 4,\n",
        "    }\n",
        ")\n",
        "output.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(url=output.output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
